{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-japanese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsnipper.io/s/-MSTCnlcnC3p7aUSnOm_\\nhttps://www.kaggle.com/c/clickbait-news-detection/\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "snipper.io/s/-MSTCnlcnC3p7aUSnOm_\n",
    "https://www.kaggle.com/c/clickbait-news-detection/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "olympic-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, NGram, HashingTF, IDF, CountVectorizer, OneHotEncoder, StringIndexer, Word2Vec\n",
    "from pyspark.sql.functions import col, udf, length, avg, lit, concat, size\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "#from pyspark.mllib.classification import NaiveBayes#, NaiveBayesModel\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tough-terminology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " \"i'll\",\n",
       " \"you'll\",\n",
       " \"he'll\",\n",
       " \"she'll\",\n",
       " \"we'll\",\n",
       " \"they'll\",\n",
       " \"i'd\",\n",
       " \"you'd\",\n",
       " \"he'd\",\n",
       " \"she'd\",\n",
       " \"we'd\",\n",
       " \"they'd\",\n",
       " \"i'm\",\n",
       " \"you're\",\n",
       " \"he's\",\n",
       " \"she's\",\n",
       " \"it's\",\n",
       " \"we're\",\n",
       " \"they're\",\n",
       " \"i've\",\n",
       " \"we've\",\n",
       " \"you've\",\n",
       " \"they've\",\n",
       " \"isn't\",\n",
       " \"aren't\",\n",
       " \"wasn't\",\n",
       " \"weren't\",\n",
       " \"haven't\",\n",
       " \"hasn't\",\n",
       " \"hadn't\",\n",
       " \"don't\",\n",
       " \"doesn't\",\n",
       " \"didn't\",\n",
       " \"won't\",\n",
       " \"wouldn't\",\n",
       " \"shan't\",\n",
       " \"shouldn't\",\n",
       " \"mustn't\",\n",
       " \"can't\",\n",
       " \"couldn't\",\n",
       " 'cannot',\n",
       " 'could',\n",
       " \"here's\",\n",
       " \"how's\",\n",
       " \"let's\",\n",
       " 'ought',\n",
       " \"that's\",\n",
       " \"there's\",\n",
       " \"what's\",\n",
       " \"when's\",\n",
       " \"where's\",\n",
       " \"who's\",\n",
       " \"why's\",\n",
       " 'would']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyspark_stopwords = StopWordsRemover().getStopWords()#https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover\n",
    "display(type(pyspark_stopwords), len(pyspark_stopwords), pyspark_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-wrist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-precipitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deadly-desire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "liked-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"no sc to stop\")\n",
    "    \n",
    "# try:\n",
    "#     spark.stop()\n",
    "# except Exception as e:\n",
    "#     print(\"{}, {}\".format(e,type(e)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "spoken-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ClickBait')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suitable-tuition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<SparkContext master=local[*] appName=PySparkShell>,\n",
       " <pyspark.sql.session.SparkSession at 0x7effec700fd0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc, spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southwest-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"file:///home/hadoop/data/clickbait/train.csv\"\n",
    "#train_path = \"/data/train.csv\"\n",
    "#train_path = \"hdfs://localhost:9000/data/train.csv;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load main training dataframe\n",
    "\n",
    "t_df = spark.read.csv(train_path, header=True, inferSchema=True).limit(5000)\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "revised-thumb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "timely-sailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df=t_df.na.drop()\n",
    "\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incoming-canadian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "killing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = t_df.withColumn(\"s\", concat(col('title'), col('text')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "postal-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   s|\n",
      "+--------------------+\n",
      "|China and Economi...|\n",
      "|Trade to Be a Big...|\n",
      "|The Top Beaches I...|\n",
      "|Sheriff’s Report ...|\n",
      "|Surgeon claiming ...|\n",
      "|This Is How Diffe...|\n",
      "|Samantha Bee and ...|\n",
      "|Krauthammer: Syri...|\n",
      "|Rust Belt voters ...|\n",
      "|As Illegal Outpos...|\n",
      "|$94.4m $198m $112...|\n",
      "|Trump’s Mar-a-Lag...|\n",
      "|STOCKS SLIP TO EN...|\n",
      "|Jared Kushner in ...|\n",
      "|He Cheated. Now H...|\n",
      "|The fabulous life...|\n",
      "|Destinations Insi...|\n",
      "|President Donald ...|\n",
      "|4 Things That You...|\n",
      "|Freddie Flintoff ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.select('s').limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dramatic-robin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|                s|\n",
      "+-----------------+\n",
      "|2500.170392749245|\n",
      "+-----------------+\n",
      "\n",
      "Help on function length in module pyspark.sql.functions:\n",
      "\n",
      "length(col)\n",
      "    Computes the character length of string data or number of bytes of binary data.\n",
      "    The length of character data includes the trailing spaces. The length of binary data\n",
      "    includes binary zeros.\n",
      "    \n",
      "    >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "    [Row(length=4)]\n",
      "    \n",
      "    .. versionadded:: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selection = ['s']\n",
    "\n",
    "t_df \\\n",
    "    .select(*(length(col(c)).alias(c) for c in selection)) \\\n",
    "    .agg(*(avg(col(c)).alias(c) for c in selection)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "finished-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "def get_avg_len_cols(df, selection):\n",
    "    assert isinstance(selection, Iterable)\n",
    "    return(\n",
    "        df \\\n",
    "        .select(*(length(col(c)).alias(c) for c in selection)) \\\n",
    "        .agg(*(avg(col(c)).alias(c) for c in selection)).show()\n",
    "              )\n",
    "\n",
    "def get_avg_size_cols(df, selection):\n",
    "    assert isinstance(selection, Iterable)\n",
    "    return(\n",
    "        df \\\n",
    "        .select(*(size(col(c)).alias(c) for c in selection)) \\\n",
    "        .agg(*(avg(col(c)).alias(c) for c in selection)).show()\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "detailed-lafayette",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Output column ss already exists.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o350.transform.\n: java.lang.IllegalArgumentException: Output column ss already exists.\n\tat org.apache.spark.ml.UnaryTransformer.transformSchema(Transformer.scala:112)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.UnaryTransformer.transform(Transformer.scala:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1d80b4fcc01f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#t_df = StopWordsRemover(inputCol = \"s\", outputCol=\"ss\").transform(t_df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_avg_len_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m's'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0msome\u001b[0m \u001b[0mpunctuation\u001b[0m \u001b[0mlike\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstill\u001b[0m \u001b[0mshowing\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Output column ss already exists.'"
     ]
    }
   ],
   "source": [
    "t_df = Tokenizer(inputCol = \"s\", outputCol = \"ss\").transform(t_df)\n",
    "\n",
    "\"\"\"\n",
    "some punctuation like ',' is still showing after Tokenizer().transform.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "comfortable-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- ss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dimensional-martin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                ss|\n",
      "+------------------+\n",
      "|415.75649546827793|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "creative-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                ss|\n",
      "+------------------+\n",
      "|415.75649546827793|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "get_avg_size_cols(t_df, ['ss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "vanilla-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = StopWordsRemover(inputCol = \"ss\", outputCol=\"sss\").transform(t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "magnetic-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|              sss|\n",
      "+-----------------+\n",
      "|251.9915407854985|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_avg_size_cols(t_df, ['sss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "earlier-southwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sss                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[china, economic, reform:, xi, jinping’s, track, record, economists, generally, agree:, china, must, overhaul, huge, wasteful, economy, wants, continue, grow, years, come., means, limiting, political, interference, banking, financial, system,, making, bloated, industries, sensitive, market, forces, lowering, barriers, foreign, trade, investment., proponents, took, heart, late, 2012, president, xi, jinping, took, formal, control, chinese, communist, party, pledges, crack, corruption, cut, red, tape., today,, mr., xi, nears, end, first, five-year, term,, much, optimism, among, economists, faded., china, remains, heavily, dependent, large, investments, maintained, brisk, slowing, economic, growth, steep, rise, government-steered, lending., still,, administration, made, small, changes,, hints, mr., xi, may, focus, economic, overhauls, starts, second, term., mr., xi, done, far, —, and,, important,, done., reforms:, experts, give, credit, china’s, moves, currency., beijing, persuaded, international, monetary, fund, 2015, admit, currency,, renminbi,, elite, club, currencies, central, banks, hold, reserves., win, approval,, china, made, much, easier, move, money, country,, shifted, daily, pegging, renminbi, dollar, slightly, market-based, system., lingering, problems:, moves, doubt, lately., opening, money, flows, led, many, chinese, families, companies, send, money, country, renminbi, weakened, dollar, chinese, government, spend, nearly, $1, trillion, prop, up., past, 12, months,, beijing, reimposed, many, restrictions, sending, money, country., reforms:, faced, early, last, year, huge, supply, unsold, real, estate, stalling, construction,, beijing, decided, make, much, easier, banks, issue, mortgages., set, buying, frenzy, big, cities, slightly, pared, backlog, empty, apartments., lingering, problems:, looked, like, bubble, looks, like, one, even, now., beijing, shanghai, already, world’s, highest, real, estate, prices, relation, local, incomes., developers, still, heavily, debt., reforms:, china, made, limited, moves, allow, foreigners, trade, extensively, bond, market,, hedge, currency, risk, connect, stock, markets, shanghai, shenzhen, hong, kong,, long, served, china’s, financial, gateway, rest, world., local, governments, discouraged, setting, companies, borrow, heavily, pay, public, works., lingering, problems:, moves, open, overshadowed, tighter, government, control, 2015, stock, market, crash., major, stock, market, index, passed, including, chinese, stocks,, citing, need, improvements., new, public-private, partnerships, emerged, continue, china’s, borrowing, spree., chinese, officials, hoped, private, partners, force, local, governments, make, wiser, cautious, investments,, initial, “private”, partners, tended, state-owned, enterprises,, typically, share, local, governments’, interest, borrowing, heavily, create, jobs., reforms:, china, moved, help, banks, plagued, rising, tide, bad, loans., banks, allowed, swap, unpaid, loans, equity, stakes, troubled, borrowers., asset, management, companies, buying, bad, loans, banks., banks, given, growing, discretion, set, interest, rates, based, creditworthiness, borrowers., interest, rates, banks, pay, deposits, deregulated,, allowing, competition, among, banks, benefits, depositors., lingering, problems:, moves, enough., banks, still, face, large, overhang, loans, money-losing, companies, little, hope, repayment., banks, continue, roll, loans, troubled, borrowers, extend, huge, loans, politically, connected, borrowers,, including, influential, private, companies, well, state-owned, enterprises., economy, slow, sharply,, mountain, bad, loans, grow, much, more., time,, entrepreneurs, continue, complain, system, denies, access, cheap, money, need, grow., reforms:, china, modestly, reduced, steel-making, coal-mining, capacity., lingering, problems:, lot, work, do., china, still, roughly, steel-making, capacity, rest, world, combined., china, still, many, coal, mines, given, long-term, plans, shift, solar,, wind, nuclear, energy., many, industrial, sectors,, intense, competition, slowing, economic, growth, curbed, private, investment., reforms:, pay, limited, top, executives., enterprises, merged,, notably, rail, equipment,, limit, extent, compete, one, another, overseas, sales., lingering, problems:, china’s, state-run, companies, remain, bloated, inefficient., monopolies, oligopolies, continue, dominate, large, sectors, economy,, like, telecommunications, power, transmission., state-owned, enterprises, sectors, like, steel, making, coal, mining, tend, focus, mainly, preserving, employment, workers,, matter, much, money, need, borrow, state-controlled, banks, cover, financial, losses., pay, limits?, may, drive, talented, leaders, private, sector., reforms:, faced, shrinking, labor, force, population, rapidly, graying,, mr., xi, ended, china’s, notorious, one-child, policy,, fines, forced, abortions,, government, even, begun, mulling, whether, offer, incentives, families, second, child., lingering, problems:, labor, force, continue, shrink, decades,, presenting, serious, drag, economic, growth., reforms:, government, made, easier, migrant, workers, rural, areas, obtain, residency, access, social, benefits, medium-size, smaller, cities., government, preparing, move, municipal, bureaucracy, beijing, end, year, outlying, suburb, part, experiment, aimed, testing, whether, build, satellite, cities, around, major, metropolitan, centers., lingering, problems:, rural, migrants, still, little, hope, gaining, residency, big, cities, like, beijing, shanghai., without, residency,, access, medical, insurance,, education, children, benefits, limited.]|\n",
      "|[trade, big, topic, theresa, may’s, u.s., visitlondon—british, prime, minister, theresa, may, said, she’ll, discuss, trade, security, coming, meeting, president, donald, trump,, first, visit, foreign, leader, president,, underscoring, significance, countries’, relationship., britain,, longtime, u.s., ally,, seeking, build, ties, u.s., leaves, european, union., mrs., may’s, key, objective, visit, washington, friday, lay, the...]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|[top, beaches, world,, according, national, geographicbeaches, come, sorts, shapes, sizes, beyond, typical, caribbean, postcard., such, national, geographic’s, new, list, top, 21, beaches, world includes, diverse, mix, shorelines, around, globe,, picture-perfect, caribbean, numbers, black-sand, beauty, iceland, shell-covered, spot, austral, coast., highlights, include:, head, national, geographic, rest, top, 21, beaches in, world., correction:, previous, version, post, incorrectly, identified, photo, lazy, beach., photo, replaced,, title, updated, reflect, lazy, beach, island, koh, rong, samloem,, koh, rong. ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|[sheriff’s, report, provides, new, details, tamir, rice’s, death,, leaves, questions, timeline, happened, tamir, rice,, 12-year-old, boy,, killed, police, officer, cleveland, last, november., lengthy, report, published, saturday, revealed, new, details, fatal, shooting, 12-year-old, tamir, rice, cleveland, police, officer,, made, recommendation, whether, officer, partner, face, criminal, charges., report, cuyahoga, county, sheriff’s, department, also, left, major, questions, unanswered., perhaps, foremost:, officer, tim, loehmann, issue, warning, firing, fatal, round?, police, said, did,, new, report, suggested, witnesses, heard, warning, shots, fired., coming, weeks, months,, grand, jury, expected, hear, evidence, case, decide, whether, bring, indictments, officer, loehmann, partner,, officer, frank, garmback., sheriff’s, investigation, provided, clearest, picture, yet, happened, nov., 22, outside, neighborhood, recreation, center, tamir, frequented,, shooting, using, newly, acquired, fake, gun, looked, strikingly, like, real, thing., tamir’s, death, led, protests, cleveland, helped, fuel, national, debate, law, enforcement, tactics, police, treat, african-americans., much, known, previously, came, surveillance, footage, recorded, shooting, circulated, online, months,, outraging, many, said, boy, opportunity, drop, replica, weapon, shot., video, shows, police, car, sliding, halt, grass, front, gazebo, outside, recreation, center., officer, loehmann,, new, force,, quickly, hopped, cruiser., standing, seven, feet, boy,, according, report,, squeezed, two, rounds, handgun, within, two, seconds, exiting, car., police, officers, arrived, scene, tense, minutes, shooting, recalled, officer, loehmann, saying, tamir, gun, reaching, it., also, told, investigators, initially, thought, tamir, looked, good, deal, older, 12., officer, william, cunningham, said, officer, loehmann, distraught., “ ‘he, gave, choice,’ ”, officer, cunningham, said, officer, loehmann, told, him,, according, report., “ ‘he, reached, gun,, nothing, do.’ ”, report, suggested, witnesses, heard, warning, shots, fired, tamir,, quoted, one, witness, saying, verbal, warning, come, two, shots, fired., witness,, named,, said, heard, “bang,, bang,”, heard, someone, yell,, “freeze!, show, hands,”, according, summary, interview, detective., that,, heard, one, “bang.”, sheriff’s, report, said, officer, loehmann, fired, twice., report, found, that,, according, witness, interviews,, “unclear, whether, officer, loehmann, shouted, verbal, commands”, patrol, car, tamir, firing, him., local, leaders, asked, judge, issue, arrest, warrants, two, cleveland, policemen, involved, 2014, fatal, shooting, 12-year-old, tamir, rice., officer, loehmann, officer, garmback, sent, scene, 911, caller, said, seen, someone, outside, recreation, center, pulling, gun, pants, scaring, people., caller,, identified,, also, told, emergency, operator,, constance, hollinger,, believed, tamir, probably, juvenile, gun, probably, fake, —, information, never, relayed, two, officers., according, new, report,, ms., hollinger’s, lawyer, said, prosecutors, told, target, criminal, prosecution., yet, interview, police, detective,, lawyer’s, advice,, refused, tell, investigators, relay, caller’s, caveats., one, provided, tamir, medical, assistance, f.b.i., agent, happened, working, nearby, arrived., agent,, also, certified, paramedic,, told, investigators, tamir, “an, incredibly, disturbing, looking, injury”, first, appeared, unresponsive., eventually,, agent, said,, tamir, became, alert,, saying, name, shot., agent, also, recalled, tamir, making, comment, gun,, sure, boy, said., rice, incident, one, several, recent, years, brought, harsh, attention, cleveland, division, police., scathing, justice, department, report, last, year, found, pattern, excessive, force, cleveland, officers,, city, settled, consent, decree, last, month, calls, heightened, monitoring, officer, accountability., agreement, came, days, officer, michael, brelo, acquitted, manslaughter, role, 2012, police, chase, ended, two, unarmed, black, people, shot, dead., police, fired, total, 137, shots, incident., cleveland, activists, expressed, skepticism, grand, jury, process,, days, ago, used, obscure, ohio, law, ask, cleveland, judge, issue, arrest, warrants, two, officers, involved, tamir’s, death., judge, thursday, found, probable, cause, arrest, counts, officers,, said, prosecutors, decide, whether, seek, warrants., timothy, j., mcginty,, cuyahoga, county, prosecutor,, said, allow, grand, jury, decide, whether, officers, arrested, charged., departure, high-profile, cases, elsewhere, involving, police,, mr., mcginty, chose, turn, investigative, documents, saturday, decision, charges,, move, said, allow, fuller, understanding, encounter., walter, madison,, lawyer, rice, family,, said, believed, mr., mcginty, likely, decided, release, documents, pressure, judge’s, finding, probable, cause, charge, officers., mr., madison, also, criticized, mr., mcginty, ordering, arrest, two, officers, judge’s, ruling., contrast,, said,, police, prosecutors, “don’t, hesitate, rely, judge, signs, search, warrant, arrest, warrant, private, citizen.”]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|[surgeon, claiming, transplant, volunteer's, head, another, body, says, needs, america's, help, itan, italian, neurosurgeon, claimed, months, perform, world's, first, human, head, transplant, asked, americans, 'be, americans', donate, cause. , dr., sergio, canavero's, plan, transplant, head, volunteer, valery, spiridonov,, whose, spinal, muscular, atrophy, disease, left, severely, disabled,, dubbed, 'reckless', number, fellow, doctors., making, headlines, months,, canavero, invited, speak, conference, for the, american, academy, neurological, orthopaedic, surgeons, annapolis, week,, spiridonov, flew, russia, join, meet, first, time., scroll, videos , leaving,, spiridonov, told, mail, online, hoped, help, promote, idea, surgery,, ultimately, persuade, medical, world, support, it,, participating, conference. , canavero, gave, little, detail, exactly, pull, operation, speaking, conference,, admitting, end, almost, three, hour, lecture, contribution, solely, dealing, spinal, cord. , specifically,, surgeon, said, reattaching, spiridonov's, spine, require, building, nano-blade, ability, cut, nerve, fibers, without, hurting, them. , also, cut, bit, lower, needed, spiridonov's, spinal, cord, bit, higher, transplant, body, giving, last-minute, second, cut,, said, help, minimize, cells, dying, severed, ends,, according, national, geographic. , surgeon, said, use, polyethylene, glycol, join, ends, together,, adding, electrical, stimulation, encourage, attachment. , questions, left, unanswered,, spiridonov's, blood, vessels, reconnected, brain, even, make, surgery, without, damage. , latter, factor, dr., raymond, dieter, said, make, successful, head, transplant, impossible., 'in, three, five, minutes,, circulation, back, brain,, dead,', cardiothoracic, surgeon, told, nbc, news. , 'when, look, inside, skull,, mush.' , regardless,, dieter, said, thought, 'phenomenal', canavero,, likens, dr., frankenstein,, thinking, outside, box. , kind, enthusiasm, canavero, hoping, convince, doctors, specialties, join, team,, promising, 'paid, nose'. , 'i, think, doctors, involved, paid, football, players,', said. , final, question, conference, showed, transplant, canavero, doctors. , reporter, asked, spiridonov, say, people, tell, surgery,, hopes, happen, two, years,, attempted. , 'maybe,', said,, 'they, imagine, place.' ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.select('sss').limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "accepting-orchestra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:42577)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n"
     ]
    },
    {
     "ename": "Py4JNetworkError",
     "evalue": "An error occurred while trying to connect to the Java server (127.0.0.1:42577)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-79581695e14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, withReplacement, fraction, seed)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwithReplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    981\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \"\"\"\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeque\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m_create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m         connection = GatewayConnection(\n\u001b[1;32m    936\u001b[0m             self.gateway_parameters, self.gateway_property)\n\u001b[0;32m--> 937\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0;34m\"server ({0}:{1})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_authenticate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: An error occurred while trying to connect to the Java server (127.0.0.1:42577)"
     ]
    }
   ],
   "source": [
    "t_df.sample(False, 0.1, seed=0).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "specific-orbit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "rotary-premium",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\".format(\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36muiWebUrl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muiWebUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;34m\"\"\"Return the URL of the SparkUI instance started by this SparkContext\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muiWebUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "continuous-majority",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "stop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d245038dc7d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplan\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mto\u001b[0m \u001b[0mturn\u001b[0m \u001b[0mthis\u001b[0m \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mStopWordsRemover\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNgram1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNgram2\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#count_tokens = udf(lambda words : len(words) , IntegerType())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: stop"
     ]
    }
   ],
   "source": [
    "# Preprocessing Strategy\n",
    "\"\"\"\n",
    "plan is to turn this into a Pipeline([Tokenizer,StopWordsRemover,Ngram1,Ngram2...])\n",
    "\"\"\"\n",
    "raise AssertionError('stop')\n",
    "\n",
    "#count_tokens = udf(lambda words : len(words) , IntegerType())\n",
    "text_cols= [\"text\",\"title\"]\n",
    "for t_c in text_cols:\n",
    "    t_df = Tokenizer(inputCol = t_c, outputCol = \"token_\"+t_c).transform(t_df)\n",
    "    t_df = StopWordsRemover(inputCol = \"token_\"+t_c, outputCol = t_c+\"_f_token\").transform(t_df)\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        t_df= NGram(n=i, inputCol=t_c+\"_f_token\", outputCol=t_c+\"_ngram_\"+str(i)).transform(t_df)\n",
    "        \n",
    "        #CountVectorizer for each ngram col\n",
    "#         c_v = CountVectorizer(inputCol= t_c+\"_ngram_\"+str(i), outputCol=t_c+\"_ngram_\"+str(i)+\"cvfeat\",\n",
    "#                               minDF=2.0, vocabSize=2**14)\n",
    "        \n",
    "        #indexer = StringIndexer(inputCol=t_c+\"_ngram_\"+str(i), outputCol=t_c+\"_ngram_\"+str(i)+\"ix_feat\")\n",
    "        #display(t_c+\"_ngram_\"+str(i), t_c+\"_ngram_\"+str(i)+\"cvfeat\")\n",
    "        \n",
    "        #m_ = c_v.fit(t_df) # c_v.fit is failing on \n",
    "        #t_df = m_.tranform(t_df)\n",
    "        #m_ = c_v.fit(t_df)\n",
    "        #t_df = m_.transform(t_df)\n",
    "    \n",
    "    t_df = t_df.drop(\"token_\"+t_c)\n",
    "\n",
    "\n",
    "\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "secure-silly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- text_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "manufactured-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing Strategy\n",
    "# text_cols= [\"text\",\"title\"]\n",
    "\n",
    "# for t_c in text_cols:\n",
    "#     t_df = Tokenizer(inputCol = t_c, outputCol = \"token_\"+t_c).transform(t_df)\n",
    "#     t_df = StopWordsRemover(inputCol = \"token_\"+t_c, outputCol = t_c+\"_f_token\").transform(t_df)\n",
    "    \n",
    "#     for i in range(1,4):\n",
    "#         t_df= NGram(n=i, inputCol=t_c+\"_f_token\", outputCol=t_c+\"_ngram_\"+str(i)).transform(t_df)\n",
    "        \n",
    "#         #CountVectorizer for each ngram col\n",
    "#         c_v = CountVectorizer(inputCol= t_c+\"_ngram_\"+str(i), outputCol=t_c+\"_ngram_\"+str(i)+\"cvfeat\",\n",
    "#                               minDF=2.0, vocabSize=2**14)\n",
    "        \n",
    "#         #indexer = StringIndexer(inputCol=t_c+\"_ngram_\"+str(i), outputCol=t_c+\"_ngram_\"+str(i)+\"ix_feat\")\n",
    "#         #display(t_c+\"_ngram_\"+str(i), t_c+\"_ngram_\"+str(i)+\"cvfeat\")\n",
    "        \n",
    "#         #m_ = c_v.fit(t_df) # c_v.fit is failing\n",
    "#         #t_df = m_.tranform(t_df)\n",
    "        \n",
    "\n",
    "#     t_df = t_df.drop(\"token_\"+t_c)\n",
    "\n",
    "\n",
    "# #t_df.show()\n",
    "\n",
    "\n",
    "# # def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
    "# #     tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "# #     ngrams = [\n",
    "# #         NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "# #         for i in range(1, n + 1)\n",
    "# #     ]\n",
    "\n",
    "# #     cv = [\n",
    "# #         CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "# #             outputCol=\"{0}_tf\".format(i))\n",
    "# #         for i in range(1, n + 1)\n",
    "# #     ]\n",
    "# #     idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "# #     assembler = [VectorAssembler(\n",
    "# #         inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "# #         outputCol=\"features\"\n",
    "# #     )]\n",
    "# #     label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "# #     lr = [LogisticRegression(maxIter=100)]\n",
    "# #     return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "periodic-visitor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{str}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#t_df.select(\"title_ngram_3\").show(5, truncate=False)\n",
    "\n",
    "t_df.select(\"text_ngram_1\").limit(1).collect()[0]['text_ngram_1']\n",
    "\n",
    "{type(z) for z in t_df.select(\"text_ngram_1\").limit(1).collect()[0]['text_ngram_1']}\n",
    "#, t_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "arbitrary-morrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- text_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-optimization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southern-chain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string, text_f_token: array<string>, text_ngram_1: array<string>, text_ngram_2: array<string>, text_ngram_3: array<string>, title_f_token: array<string>, title_ngram_1: array<string>, title_ngram_2: array<string>, title_ngram_3: array<string>, 999998: vector]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_v = CountVectorizer(inputCol=\"text_ngram_2\", outputCol=\"999998\",\n",
    "                     minDF=2.0, vocabSize=2**14)\n",
    "\n",
    "# _p = {'inputCol':\"title_ngram_1\", 'outputCol':\"title_ngram_1_cv\",\n",
    "#       'minDF':2.0, 'vocabSize':2**14}\n",
    "\n",
    "#m_ = c_v.fit(t_df, params={'inputCol': 'text_ngram_1'})\n",
    "mdl = c_v.fit(t_df)\n",
    "\n",
    "t_df=mdl.transform(t_df)\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "coral-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame(\n",
    "#     [(0, [\"a\", \"b a\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "#     [\"label\", \"raw\"])\n",
    "# cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "# model = cv.fit(df)\n",
    "# model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "superb-framing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- text_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 999998: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " ['id',\n",
       "  'title',\n",
       "  'text',\n",
       "  'label',\n",
       "  'text_f_token',\n",
       "  'text_ngram_1',\n",
       "  'text_ngram_2',\n",
       "  'text_ngram_3',\n",
       "  'title_f_token',\n",
       "  'title_ngram_1',\n",
       "  'title_ngram_2',\n",
       "  'title_ngram_3',\n",
       "  '999998'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.printSchema(), t_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sufficient-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module pyspark.ml.feature:\n",
      "\n",
      "class CountVectorizer(pyspark.ml.wrapper.JavaEstimator, _CountVectorizerParams, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  Extracts a vocabulary from document collections and generates a :py:attr:`CountVectorizerModel`.\n",
      " |  \n",
      " |  >>> df = spark.createDataFrame(\n",
      " |  ...    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
      " |  ...    [\"label\", \"raw\"])\n",
      " |  >>> cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> model = cv.fit(df)\n",
      " |  >>> model.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  >>> sorted(model.vocabulary) == ['a', 'b', 'c']\n",
      " |  True\n",
      " |  >>> countVectorizerPath = temp_path + \"/count-vectorizer\"\n",
      " |  >>> cv.save(countVectorizerPath)\n",
      " |  >>> loadedCv = CountVectorizer.load(countVectorizerPath)\n",
      " |  >>> loadedCv.getMinDF() == cv.getMinDF()\n",
      " |  True\n",
      " |  >>> loadedCv.getMinTF() == cv.getMinTF()\n",
      " |  True\n",
      " |  >>> loadedCv.getVocabSize() == cv.getVocabSize()\n",
      " |  True\n",
      " |  >>> modelPath = temp_path + \"/count-vectorizer-model\"\n",
      " |  >>> model.save(modelPath)\n",
      " |  >>> loadedModel = CountVectorizerModel.load(modelPath)\n",
      " |  >>> loadedModel.vocabulary == model.vocabulary\n",
      " |  True\n",
      " |  >>> fromVocabModel = CountVectorizerModel.from_vocabulary([\"a\", \"b\", \"c\"],\n",
      " |  ...     inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> fromVocabModel.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  \n",
      " |  .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      _CountVectorizerParams\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      __init__(self, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18, binary=False,                 inputCol=None,outputCol=None)\n",
      " |  \n",
      " |  setBinary(self, value)\n",
      " |      Sets the value of :py:attr:`binary`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setMaxDF(self, value)\n",
      " |      Sets the value of :py:attr:`maxDF`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setMinDF(self, value)\n",
      " |      Sets the value of :py:attr:`minDF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setMinTF(self, value)\n",
      " |      Sets the value of :py:attr:`minTF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setParams(self, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      setParams(self, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18, binary=False,                  inputCol=None, outputCol=None)\n",
      " |      Set the params for the CountVectorizer\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setVocabSize(self, value)\n",
      " |      Sets the value of :py:attr:`vocabSize`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  getBinary(self)\n",
      " |      Gets the value of binary or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  getMaxDF(self)\n",
      " |      Gets the value of maxDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  getMinDF(self)\n",
      " |      Gets the value of minDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getMinTF(self)\n",
      " |      Gets the value of minTF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getVocabSize(self)\n",
      " |      Gets the value of vocabSize or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  binary = Param(parent='undefined', name='binary', doc='Bi...vents rath...\n",
      " |  \n",
      " |  maxDF = Param(parent='undefined', name='maxDF', doc='Spe...ts the term...\n",
      " |  \n",
      " |  minDF = Param(parent='undefined', name='minDF', doc='Spe...pecifies th...\n",
      " |  \n",
      " |  minTF = Param(parent='undefined', name='minTF', doc=\"Fil...rModel and ...\n",
      " |  \n",
      " |  vocabSize = Param(parent='undefined', name='vocabSize', doc='max size ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n",
      " |                     param maps is given, this calls fit on each param map and returns a list of\n",
      " |                     models.\n",
      " |      :returns: fitted model(s)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n",
      " |      :param paramMaps: A Sequence of param maps.\n",
      " |      :return: A thread safe iterable which contains one model for each param map. Each\n",
      " |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |               using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |      \n",
      " |      .. note:: DeveloperApi\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text_tokenizer = Tokenizer(inputCol = 'text', outputCol = 'text_words')\n",
    "#ttl_tokenizer = Tokenizer(inputCol = 'title', outputCol = 'ttl_words')\n",
    "#regex_tokenizer = RegexTokenizer(inputCol= 'sentence', outputCol= 'words', pattern='\\\\W')\n",
    "#t_df = tokenizer.transform(t_df)\n",
    "# t_df = text_tokenizer.transform(ttl_tokenizer.transform(t_df))\n",
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "alleged-promise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string, text_f_token: array<string>, text_ngram_1: array<string>, text_ngram_2: array<string>, text_ngram_3: array<string>, title_f_token: array<string>, title_ngram_1: array<string>, title_ngram_2: array<string>, title_ngram_3: array<string>, 999998: vector, label_index: double, features: vector]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Feature building\n",
    "\n",
    "fcols = [\n",
    "#  'text_f_token',\n",
    "  'text_ngram_1',\n",
    "  'text_ngram_2',\n",
    "  'text_ngram_3',\n",
    "#  'title_f_token',\n",
    "  'title_ngram_1',\n",
    "  'title_ngram_2',\n",
    "  'title_ngram_3']\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "t_df = indexer.fit(t_df).transform(t_df)\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"text_ngram_2\", outputCol=\"features\")\n",
    "t_df=cv.fit(t_df).transform(t_df)\n",
    "\n",
    "t_df.persist()\n",
    "\n",
    "\n",
    "\n",
    "# ohe = OneHotEncoderEstimator(\n",
    "#     inputCols= fcols,\n",
    "#     outputCols=[\"feat_\"+s for s in fcols])\n",
    "\n",
    "#f_df=ohe.fit(t_df).transform(t_df)\n",
    "    \n",
    "#f_df=cv.fit(t_df).transform(t_df)\n",
    "#OneHotEncoderEstimator\n",
    "#hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aware-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- text_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- text_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- text_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_f_token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title_ngram_1: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_2: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- title_ngram_3: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 999998: vector (nullable = true)\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# idf= IDF(inputCol='rawFeatures', outputCol='features')\n",
    "# idf_model = idf.fit(featurized_df)\n",
    "# rescaled_df = idf_model.transform(featurized_df)\n",
    "\n",
    "# rescaled_df.show(truncate=False)\n",
    "\n",
    "t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rough-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class NaiveBayes in module pyspark.ml.classification:\n",
      "\n",
      "class NaiveBayes(pyspark.ml.wrapper.JavaEstimator, pyspark.ml.param.shared.HasFeaturesCol, pyspark.ml.param.shared.HasLabelCol, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasProbabilityCol, pyspark.ml.param.shared.HasRawPredictionCol, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  Naive Bayes Classifiers.\n",
      " |  It supports both Multinomial and Bernoulli NB. `Multinomial NB\n",
      " |  <http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html>`_\n",
      " |  can handle finitely supported discrete data. For example, by converting documents into\n",
      " |  TF-IDF vectors, it can be used for document classification. By making every vector a\n",
      " |  binary (0/1) data, it can also be used as `Bernoulli NB\n",
      " |  <http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html>`_.\n",
      " |  The input feature values must be nonnegative.\n",
      " |  \n",
      " |  >>> from pyspark.sql import Row\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     Row(label=0.0, weight=0.1, features=Vectors.dense([0.0, 0.0])),\n",
      " |  ...     Row(label=0.0, weight=0.5, features=Vectors.dense([0.0, 1.0])),\n",
      " |  ...     Row(label=1.0, weight=1.0, features=Vectors.dense([1.0, 0.0]))])\n",
      " |  >>> nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", weightCol=\"weight\")\n",
      " |  >>> model = nb.fit(df)\n",
      " |  >>> model.pi\n",
      " |  DenseVector([-0.81..., -0.58...])\n",
      " |  >>> model.theta\n",
      " |  DenseMatrix(2, 2, [-0.91..., -0.51..., -0.40..., -1.09...], 1)\n",
      " |  >>> test0 = sc.parallelize([Row(features=Vectors.dense([1.0, 0.0]))]).toDF()\n",
      " |  >>> result = model.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  1.0\n",
      " |  >>> result.probability\n",
      " |  DenseVector([0.32..., 0.67...])\n",
      " |  >>> result.rawPrediction\n",
      " |  DenseVector([-1.72..., -0.99...])\n",
      " |  >>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF()\n",
      " |  >>> model.transform(test1).head().prediction\n",
      " |  1.0\n",
      " |  >>> nb_path = temp_path + \"/nb\"\n",
      " |  >>> nb.save(nb_path)\n",
      " |  >>> nb2 = NaiveBayes.load(nb_path)\n",
      " |  >>> nb2.getSmoothing()\n",
      " |  1.0\n",
      " |  >>> model_path = temp_path + \"/nb_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = NaiveBayesModel.load(model_path)\n",
      " |  >>> model.pi == model2.pi\n",
      " |  True\n",
      " |  >>> model.theta == model2.theta\n",
      " |  True\n",
      " |  >>> nb = nb.setThresholds([0.01, 10.00])\n",
      " |  >>> model3 = nb.fit(df)\n",
      " |  >>> result = model3.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  0.0\n",
      " |  \n",
      " |  .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      NaiveBayes\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasProbabilityCol\n",
      " |      pyspark.ml.param.shared.HasRawPredictionCol\n",
      " |      pyspark.ml.param.shared.HasThresholds\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)\n",
      " |      __init__(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0,                  modelType=\"multinomial\", thresholds=None, weightCol=None)\n",
      " |  \n",
      " |  getModelType(self)\n",
      " |      Gets the value of modelType or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  getSmoothing(self)\n",
      " |      Gets the value of smoothing or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setModelType(self, value)\n",
      " |      Sets the value of :py:attr:`modelType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setParams(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)\n",
      " |      setParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0,                   modelType=\"multinomial\", thresholds=None, weightCol=None)\n",
      " |      Sets params for Naive Bayes.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setSmoothing(self, value)\n",
      " |      Sets the value of :py:attr:`smoothing`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  modelType = Param(parent='undefined', name='modelType', doc=...d optio...\n",
      " |  \n",
      " |  smoothing = Param(parent='undefined', name='smoothing', doc=...thing p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n",
      " |                     param maps is given, this calls fit on each param map and returns a list of\n",
      " |                     models.\n",
      " |      :returns: fitted model(s)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n",
      " |      :param paramMaps: A Sequence of param maps.\n",
      " |      :return: A thread safe iterable which contains one model for each param map. Each\n",
      " |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |               using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |      \n",
      " |      .. note:: DeveloperApi\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self)\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  setFeaturesCol(self, value)\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self)\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  setLabelCol(self, value)\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self)\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  setPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  getProbabilityCol(self)\n",
      " |      Gets the value of probabilityCol or its default value.\n",
      " |  \n",
      " |  setProbabilityCol(self, value)\n",
      " |      Sets the value of :py:attr:`probabilityCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  getRawPredictionCol(self)\n",
      " |      Gets the value of rawPredictionCol or its default value.\n",
      " |  \n",
      " |  setRawPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`rawPredictionCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n",
      " |  \n",
      " |  getThresholds(self)\n",
      " |      Gets the value of thresholds or its default value.\n",
      " |  \n",
      " |  setThresholds(self, value)\n",
      " |      Sets the value of :py:attr:`thresholds`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n",
      " |  \n",
      " |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self)\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  setWeightCol(self, value)\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(NaiveBayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "double-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = t_df.randomSplit([.8,.2], seed = 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "elementary-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes(modelType= \"multinomial\", labelCol=\"label_index\", featuresCol =\"999998\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "parliamentary-amount",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o459.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-39dfbf349b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_model\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb_predictions\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o459.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:176)\n\tat org.apache.spark.ml.classification.NaiveBayes$$anonfun$trainWithLabelCheck$1.apply(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n\tat org.apache.spark.ml.classification.NaiveBayes.trainWithLabelCheck(NaiveBayes.scala:129)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:118)\n\tat org.apache.spark.ml.classification.NaiveBayes.train(NaiveBayes.scala:78)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:456)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "nb_model= NB.fit(train)\n",
    "nb_predictions= nb_model.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Py4JJavaError: An error occurred while calling o459.fit.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol= \"label_index\", predictionCol = \"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(\"Accuracy : \", nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://spark.apache.org/docs/2.2.0/ml-pipeline.html\n",
    "\n",
    "Plan:\n",
    "    - use Pipeline([transformer_n, estimator_n...])\n",
    "    - Improve preprocessing (punctuation, special characters, stemming)\n",
    "    - Include transformers and estimators for each n_gram length\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
