{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pregnant-japanese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsnipper.io/s/-MSTCnlcnC3p7aUSnOm_\\nhttps://www.kaggle.com/c/clickbait-news-detection/\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "snipper.io/s/-MSTCnlcnC3p7aUSnOm_\n",
    "https://www.kaggle.com/c/clickbait-news-detection/\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olympic-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StopWordsRemover, NGram, HashingTF, IDF, CountVectorizer, OneHotEncoder, StringIndexer, Word2Vec\n",
    "from pyspark.sql.functions import col, udf, length, avg, lit, concat, size, array\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "#from pyspark.mllib.classification import NaiveBayes#, NaiveBayesModel\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liked-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print(\"no sc to stop\")\n",
    "    \n",
    "# try:\n",
    "#     spark.stop()\n",
    "# except Exception as e:\n",
    "#     print(\"{}, {}\".format(e,type(e)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deadly-desire",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;34m<\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \"\"\".format(\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36muiWebUrl\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muiWebUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;34m\"\"\"Return the URL of the SparkUI instance started by this SparkContext\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muiWebUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spoken-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ClickBait')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "alleged-manitoba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ClickBait</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff51b473630>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tough-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark_stopwords = StopWordsRemover().getStopWords()#https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.StopWordsRemover\n",
    "# display(type(pyspark_stopwords), len(pyspark_stopwords), pyspark_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southwest-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"file:///home/hadoop/data/clickbait/train.csv\"\n",
    "#train_path = \"/data/train.csv\"\n",
    "#train_path = \"hdfs://localhost:9000/data/train.csv;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "linear-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load main training dataframe\n",
    "\n",
    "t_df = spark.read.csv(train_path, header=True, inferSchema=True, encoding=\"utf-8\").limit(5000)\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "revised-thumb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "timely-sailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df=t_df.na.drop()\n",
    "\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "incoming-canadian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "under-substance",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from string import punctuation\n",
    "\n",
    "def strip_possessive(l_):\n",
    "    ptrn=re.compile(r\"['`’]s\", flags=re.I)\n",
    "    r__ = [ptrn.sub('', s_) for s_ in l_]\n",
    "    return(r__)  \n",
    "\n",
    "@udf(\"string\")\n",
    "def strip_possessive_str(s_):\n",
    "    ptrn=re.compile(r\"['`’]s\", flags=re.I)\n",
    "    r__ = ptrn.sub(' ', s_)\n",
    "    return(r__)\n",
    "\n",
    "\n",
    "def strip_func(l_):\n",
    "    ptrn = re.compile('[\\W_]+')\n",
    "    r__ = [ptrn.sub('', s_) for s_ in l_]\n",
    "    return(r__)\n",
    "\n",
    "@udf(\"string\")\n",
    "def strip_func_str(s_):\n",
    "    ptrn = re.compile('[\\W_]+')\n",
    "    r__ = ptrn.sub(' ', s_)\n",
    "    return(r__)\n",
    "\n",
    "#udf_strip_func = udf(strip_func, ArrayType(StringType()))\n",
    "\n",
    "# def strip_pos_spchars(l_):\n",
    "#     ptrn_pos = re.compile(r\"[']s\", flags=re.I)\n",
    "#     ptrn_spchars = re.compile('[\\W_]+')\n",
    "#     r__ = [ptrn.sub('', s_) for s_ in l_]\n",
    "#     [ptrn_spchars.sub('', ptrn_pos.sub('', s_)) for s_ in l_]\n",
    "#     return(r__)     \n",
    "#x = list(one_sss_list.take(1)[0].asDict()['sss'])\n",
    "\n",
    "#x = [\"World's fair\", \"Biden's administration\", \"fergi's song\", \"Xi's policy\"]\n",
    "\n",
    "#x, strip_func(x), strip_possessive(x), #strip_possessive_str(\"World's fair ... Biden's administration, Fergi's song. Xi's policy\")\n",
    "#strip_func(one_sss_list.take(1)[0].asDict()['sss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "impressed-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function udf in module pyspark.sql.functions:\n",
      "\n",
      "udf(f=None, returnType=StringType)\n",
      "    Creates a user defined function (UDF).\n",
      "    \n",
      "    .. note:: The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "    \n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> import random\n",
      "    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "    \n",
      "    .. note:: The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "    \n",
      "    .. note:: The user-defined functions do not take keyword arguments on the calling side.\n",
      "    \n",
      "    :param f: python function if used as a standalone function\n",
      "    :param returnType: the return type of the user-defined function. The value can be either a\n",
      "        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "    \n",
      "    >>> from pyspark.sql.types import IntegerType\n",
      "    >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "    >>> @udf\n",
      "    ... def to_upper(s):\n",
      "    ...     if s is not None:\n",
      "    ...         return s.upper()\n",
      "    ...\n",
      "    >>> @udf(returnType=IntegerType())\n",
      "    ... def add_one(x):\n",
      "    ...     if x is not None:\n",
      "    ...         return x + 1\n",
      "    ...\n",
      "    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "    +----------+--------------+------------+\n",
      "    |slen(name)|to_upper(name)|add_one(age)|\n",
      "    +----------+--------------+------------+\n",
      "    |         8|      JOHN DOE|          22|\n",
      "    +----------+--------------+------------+\n",
      "    \n",
      "    .. versionadded:: 1.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "solar-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(\"english\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# def stem_lem(w_, pos='n'):\n",
    "#     return(lemmatizer.lemmatize(stemmer.stem(w_), pos=pos))\n",
    "\n",
    "# stem_lem_cases= [\"writing\", \"write\", \"wrote\", \"writing\", \"written\", \"drag\", \"dragged\", \"drug\", \"dragon\"]\n",
    "\n",
    "# display(\n",
    "#     stem_lem_cases,\n",
    "#     [stemmer.stem(w) for w in stem_lem_cases],\n",
    "#     [lemmatizer.lemmatize(w, pos='n') for w in stem_lem_cases],\n",
    "#     [stem_lem(w) for w in stem_lem_cases]\n",
    "#     )\n",
    "\n",
    "#stem_lem = udf(stem_lem, StringType())\n",
    "\n",
    "@udf(ArrayType(StringType()))\n",
    "def stem_lem(l_, pos='n'):\n",
    "    return([lemmatizer.lemmatize(stemmer.stem(w_), pos=pos) for w_ in l_])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "killing-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = t_df.withColumn(\"s\", concat(col('title'), col('text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "postal-kruger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                   s|\n",
      "+--------------------+\n",
      "|China and Economi...|\n",
      "|Trade to Be a Big...|\n",
      "|The Top Beaches I...|\n",
      "|Sheriff’s Report ...|\n",
      "|Surgeon claiming ...|\n",
      "|This Is How Diffe...|\n",
      "|Samantha Bee and ...|\n",
      "|Krauthammer: Syri...|\n",
      "|Rust Belt voters ...|\n",
      "|As Illegal Outpos...|\n",
      "|$94.4m $198m $112...|\n",
      "|Trump’s Mar-a-Lag...|\n",
      "|STOCKS SLIP TO EN...|\n",
      "|Jared Kushner in ...|\n",
      "|He Cheated. Now H...|\n",
      "|The fabulous life...|\n",
      "|Destinations Insi...|\n",
      "|President Donald ...|\n",
      "|4 Things That You...|\n",
      "|Freddie Flintoff ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.select('s').limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "annual-envelope",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|s                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|China and Economic Reform Xi Jinping Track Record Economists generally agree China must overhaul its huge but wasteful economy if it wants to continue to grow in the years to come That means limiting political interference in banking and the financial system making bloated industries more sensitive to market forces and lowering barriers against foreign trade and investment Proponents took heart in late 2012 when President Xi Jinping took formal control of the Chinese Communist Party with pledges to crack down on corruption and cut red tape Today as Mr Xi nears the end of his first five year term much of the optimism among economists has faded China remains heavily dependent on large investments and has maintained brisk but slowing economic growth only with a steep rise in government steered lending Still his administration has made some small changes and there have been hints that Mr Xi may focus more on economic overhauls when he starts his second term Here is what Mr Xi has done so far and more important what he has not done Reforms Experts give some credit for China moves on currency Beijing persuaded the International Monetary Fund in 2015 to admit its currency the renminbi to an elite club of currencies in which central banks hold their reserves To win approval China made it much easier to move money in and out of the country and shifted the daily pegging of the renminbi to the dollar to a slightly more market based system Lingering problems Those moves have been in doubt lately Opening up money flows led so many Chinese families and companies to send their money out of the country that the renminbi weakened against the dollar and the Chinese government had to spend nearly 1 trillion to prop it up In the past 12 months Beijing has reimposed many restrictions on sending money out of the country Reforms Faced early last year with a huge supply of unsold real estate and stalling construction Beijing decided to make it much easier for banks to issue mortgages This set off a buying frenzy in big cities that slightly pared the backlog of empty apartments Lingering problems What looked like a bubble before looks like one even more so now Beijing and Shanghai already have some of the world highest real estate prices in relation to local incomes Developers are still heavily in debt Reforms China made limited moves to allow foreigners to trade more extensively in the bond market hedge their currency risk and connect its stock markets in Shanghai and Shenzhen with Hong Kong which has long served as China financial gateway to the rest of the world Local governments have been discouraged from setting up companies that borrow heavily to pay for public works Lingering problems Any moves to open up have been overshadowed by tighter government control after a 2015 stock market crash A major stock market index passed on including Chinese stocks citing the need for further improvements New public private partnerships have emerged to continue China borrowing spree While Chinese officials had hoped that the private partners would force local governments to make wiser and more cautious investments the initial private partners have tended to be state owned enterprises which typically share local governments interest in borrowing heavily to create jobs Reforms China has moved to help banks plagued by a rising tide of bad loans Banks have been allowed to swap a few of those unpaid loans for equity stakes in troubled borrowers Asset management companies have been buying some bad loans from banks Banks have been given growing discretion to set interest rates based on the creditworthiness of borrowers The interest rates that banks pay on deposits have been deregulated allowing a competition among banks that benefits depositors Lingering problems Those moves are not enough Banks still face a large overhang of loans to money losing companies with little hope of repayment Banks continue to roll over loans to troubled borrowers and extend huge loans to politically connected borrowers including influential private companies as well as state owned enterprises If the economy does slow sharply the mountain of bad loans will grow much more At the same time entrepreneurs continue to complain that the system denies them the access to cheap money that they need to grow Reforms China has modestly reduced its steel making and coal mining capacity Lingering problems It has a lot more work to do China still has roughly the same steel making capacity as the rest of the world combined China still has too many coal mines given its long term plans to shift to more solar wind and nuclear energy In many other industrial sectors intense competition and slowing economic growth have curbed private investment Reforms Pay has been limited for top executives A few enterprises have been merged notably in rail equipment to limit the extent to which they compete with one another for overseas sales Lingering problems China state run companies remain bloated and inefficient Monopolies and oligopolies continue to dominate large sectors of the economy like telecommunications and power transmission State owned enterprises in sectors like steel making and coal mining tend to focus mainly on preserving employment for their workers no matter how much money they need to borrow from state controlled banks to cover financial losses And those pay limits They may drive talented leaders to the private sector Reforms Faced with a shrinking labor force and a population that is rapidly graying Mr Xi ended China notorious one child policy with its fines and forced abortions and his government has even begun mulling whether to offer incentives for families to have a second child Lingering problems The labor force will continue to shrink for decades presenting a serious drag on economic growth Reforms The government has made it easier for migrant workers from rural areas to obtain residency and access to social benefits in medium size and smaller cities The government is preparing to move the municipal bureaucracy of Beijing at the end of this year to an outlying suburb as part of an experiment aimed at testing whether it can build satellite cities around major metropolitan centers Lingering problems Rural migrants still have little hope of gaining residency in big cities like Beijing and Shanghai Without residency their access to medical insurance education for their children and other benefits is limited |\n",
      "|Trade to Be a Big Topic in Theresa May U S VisitLONDON British Prime Minister Theresa May said she ll discuss trade and security in a coming meeting with President Donald Trump his first visit from a foreign leader as president underscoring the significance of their countries relationship Britain a longtime U S ally is seeking to build on its ties with the U S as it leaves the European Union Mrs May key objective on her visit to Washington on Friday will be to lay the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|The Top Beaches In The World According To National GeographicBeaches come in all sorts of shapes and sizes beyond the typical Caribbean postcard As such National Geographic new list of the Top 21 Beaches in the World includes a diverse mix of shorelines around the globe from those picture perfect Caribbean numbers to a black sand beauty in Iceland to a shell covered spot on the Austral coast Highlights include Head over to National Geographic for the rest of the top 21 beaches in the world CORRECTION A previous version of this post incorrectly identified a photo as Lazy Beach The photo has been replaced and the title has been updated to reflect that Lazy Beach is on the island of Koh Rong Samloem not Koh Rong                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|Sheriff Report Provides New Details on Tamir Rice Death but Leaves Questions A timeline of what happened after Tamir Rice a 12 year old boy was killed by a police officer in Cleveland last November A lengthy report published Saturday revealed new details about the fatal shooting of 12 year old Tamir Rice by a Cleveland police officer but made no recommendation on whether the officer or his partner should face criminal charges The report by the Cuyahoga County Sheriff Department also left some major questions unanswered Perhaps foremost Did Officer Tim Loehmann issue a warning before firing the fatal round The police have said he did but the new report suggested no witnesses heard any warning before shots were fired In the coming weeks or months a grand jury is expected to hear evidence in the case and decide whether to bring indictments against Officer Loehmann and his partner Officer Frank Garmback But the sheriff investigation provided the clearest picture yet about what happened on Nov 22 outside the neighborhood recreation center that Tamir frequented and where just before the shooting he had been using a newly acquired fake gun that looked strikingly like the real thing Tamir death led to protests in Cleveland and helped fuel a national debate about law enforcement tactics and how police treat African Americans Much of what had been known previously came from surveillance footage that recorded the shooting and has circulated online for months outraging many who said the boy had no opportunity to drop the replica weapon before he was shot The video shows the police car sliding to a halt on the grass in front of a gazebo outside the recreation center Officer Loehmann who was new to the force quickly hopped out of the cruiser Standing no more than seven feet from the boy according to the report he squeezed off two rounds from his handgun within two seconds of exiting the car Some police officers who arrived at the scene in the tense minutes after the shooting recalled Officer Loehmann saying that Tamir had a gun and had been reaching for it They also told investigators that they initially thought Tamir looked a good deal older than 12 Officer William Cunningham said Officer Loehmann was distraught He gave me no choice Officer Cunningham said Officer Loehmann told him according to the report He reached for the gun and there was nothing I could do The report suggested that no witnesses heard any warning before shots were fired at Tamir and it quoted one witness as saying a verbal warning had come only after two shots were fired The witness who was not named said she heard bang bang and then heard someone yell Freeze Show me your hands according to a summary of her interview with a detective After that she then heard one more bang The sheriff report said Officer Loehmann only fired twice The report found that according to witness interviews it was unclear whether Officer Loehmann shouted verbal commands from his patrol car to Tamir before firing on him Local leaders asked a judge to issue arrest warrants for two Cleveland policemen involved in the 2014 fatal shooting of 12 year old Tamir Rice Officer Loehmann and Officer Garmback were sent to the scene after a 911 caller said he had seen someone outside the recreation center pulling a gun in and out of his pants and scaring people But the caller who was not identified also told the emergency operator Constance Hollinger that he believed Tamir was probably a juvenile and his gun probably fake information that was never relayed to the two officers According to the new report Ms Hollinger lawyer said prosecutors have told her that she is not a target of criminal prosecution Yet during her interview with a police detective on her lawyer advice she refused to tell investigators why she did not relay the caller caveats No one provided Tamir medical assistance until an F B I agent who happened to be working nearby arrived That agent also a certified paramedic told investigators that Tamir had an incredibly disturbing looking injury and at first appeared unresponsive But eventually the agent said Tamir became alert saying his name and that he had been shot The agent also recalled Tamir making some comment about a gun but he was not sure what the boy said The Rice incident is one of several in recent years that have brought harsh attention to the Cleveland Division of Police A scathing Justice Department report last year found a pattern of excessive force by Cleveland officers and the city settled on a consent decree last month that calls for heightened monitoring and officer accountability That agreement came just days after Officer Michael Brelo was acquitted of manslaughter for his role in a 2012 police chase that ended with two unarmed black people shot dead The police fired a total of 137 shots in that incident Some Cleveland activists have expressed skepticism about the grand jury process and days ago they used an obscure Ohio law to ask a Cleveland judge to issue arrest warrants for the two officers involved in Tamir death The judge on Thursday found probable cause for an arrest on some counts against both officers but said prosecutors would have to decide whether to seek warrants Timothy J McGinty the Cuyahoga County prosecutor has said he will allow the grand jury to decide whether the officers should be arrested and charged In a departure from high profile cases elsewhere involving the police Mr McGinty chose to turn over the investigative documents Saturday before the decision on charges a move he said would allow for a fuller understanding of the encounter Walter Madison a lawyer for the Rice family said he believed that Mr McGinty most likely decided to release the documents because of pressure after the judge finding that there was probable cause to charge both officers Mr Madison also criticized Mr McGinty for not ordering the arrest of the two officers after the judge ruling By contrast he said the police and prosecutors don t hesitate to rely on the judge when he signs a search warrant or arrest warrant for a private citizen                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Surgeon claiming he will transplant volunteer HEAD to another body says he needs America help to do itAn Italian neurosurgeon who has claimed for months that he will perform the world first human head transplant asked Americans to be Americans and donate to his cause Dr Sergio Canavero plan to transplant the head of volunteer Valery Spiridonov whose spinal muscular atrophy disease left him severely disabled has been dubbed reckless by a number of fellow doctors But after making headlines for months Canavero was invited to speak at a conference for the American Academy of Neurological and Orthopaedic Surgeons in Annapolis this week and Spiridonov flew from Russia to both join and meet him for the first time Scroll down for videos Before leaving Spiridonov told Mail Online he hoped he could help promote the idea of the surgery and ultimately persuade the medical world to support it by participating in the conference Canavero gave little detail of just how exactly he would pull off the operation while speaking at the conference admitting at the end of an almost three hour lecture that his contribution would solely be dealing with the spinal cord Specifically the surgeon said reattaching Spiridonov spine would require building a nano blade with the ability to cut through nerve fibers without hurting them He would also cut a bit lower than needed on Spiridonov spinal cord and a bit higher on the transplant body before giving them a last minute second cut which he said would help minimize cells dying off from the severed ends according to the National Geographic The surgeon said he would then use polyethylene glycol to join the ends together adding electrical stimulation to encourage attachment But other questions were left unanswered such as how Spiridonov blood vessels would be reconnected or if the brain could even make it through the surgery without damage The latter is a factor that Dr Raymond Dieter said will make a successful head transplant impossible In three to five minutes if we don t have circulation back to your brain you re dead the cardiothoracic surgeon told NBC News When you look inside the skull it mush Regardless Dieter said he thought it was phenomenal that Canavero who likens himself to Dr Frankenstein is thinking outside of the box It is that kind of enthusiasm Canavero is hoping will convince doctors of all specialties to join his team promising them they will be paid through the nose I think doctors involved in this should be paid more than football players he said But it was the final question at the conference that showed that the transplant was not about Canavero or his doctors A reporter asked Spiridonov what he would say to people who tell him this surgery which he hopes will happen in two years should not be attempted Maybe he said they should imagine themselves in my place                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df = t_df.withColumn(\"s\", strip_func_str(strip_possessive_str(col('s'))))\n",
    "\n",
    "\n",
    "t_df.select('s').limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "institutional-bradley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"’\"==\"`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dramatic-robin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|                 s|\n",
      "+------------------+\n",
      "|2423.5222557905336|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selection = ['s']\n",
    "\n",
    "t_df \\\n",
    "    .select(*(length(col(c)).alias(c) for c in selection)) \\\n",
    "    .agg(*(avg(col(c)).alias(c) for c in selection)).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "finished-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "def get_avg_len_cols(df, selection):\n",
    "    assert isinstance(selection, Iterable)\n",
    "    return(\n",
    "        df \\\n",
    "        .select(*(length(col(c)).alias(c) for c in selection)) \\\n",
    "        .agg(*(avg(col(c)).alias(c) for c in selection)).show()\n",
    "              )\n",
    "\n",
    "def get_avg_size_cols(df, selection):\n",
    "    assert isinstance(selection, Iterable)\n",
    "    return(\n",
    "        df \\\n",
    "        .select(*(size(col(c)).alias(c) for c in selection)) \\\n",
    "        .agg(*(avg(col(c)).alias(c) for c in selection)).show()\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "detailed-lafayette",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsome punctuation like ',' is still showing after Tokenizer().transform.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df = Tokenizer(inputCol = \"s\", outputCol = \"ss\").transform(t_df)\n",
    "\n",
    "\"\"\"\n",
    "some punctuation like ',' is still showing after Tokenizer().transform.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "comfortable-explorer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- ss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dimensional-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_df.select(\"ss\").limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "creative-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_size_cols(t_df, ['ss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "vanilla-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = StopWordsRemover(inputCol = \"ss\", outputCol=\"sss\").transform(t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "magnetic-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_avg_size_cols(t_df, ['sss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "earlier-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_df.select('sss').limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "accepting-orchestra",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Column.getItem of Column<b'sss'>>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.sample(False, 0.1, seed=0).limit(1).sss.getItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "specific-orbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------+\n",
      "|sss                                                                                               |\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "|[, asap, ferg, remy, ma, rep, east, coast, new, song, asap, ferg, repping, east, coast, new, song]|\n",
      "+--------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_df.sample(False, 0.1, seed=0).select(\"sss\").limit(1).show(truncate=False)\n",
    "\n",
    "one_sss_list=t_df.sample(False, 0.1, seed=0).select(\"sss\").limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rotary-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dir(one_sss_list),\n",
    "# dir(one_sss_list.take(1)[0])\n",
    "# one_sss_list.take(1)[0].asDict()['sss']\n",
    "#stem_lem(t_df.sample(False, 0.1, seed=0).select(\"sss\").limit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "graduate-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = t_df.withColumn(\"ssss\", stem_lem(col(\"sss\")))\n",
    "#t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ultimate-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_df.select('sss', \"ssss\").limit(5).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "continuous-majority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string, s: string, ss: array<string>, sss: array<string>, ssss: array<string>, ssss_1gram: array<string>, ssss_2gram: array<string>, ssss_3gram: array<string>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NGram Creation\n",
    "\"\"\"\n",
    "plan would be to turn this into a Pipeline([Tokenizer,StopWordsRemover,Ngram1,Ngram2...])\n",
    "\"\"\"\n",
    "#raise AssertionError('stop')\n",
    "text_cols= [\"ssss\"]\n",
    "for t_c in text_cols:\n",
    "    for i in range(1,4):\n",
    "        t_df= NGram(n=i, inputCol=t_c, outputCol=t_c+\"_\"+str(i)+\"gram\").transform(t_df)\n",
    "\n",
    "\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "secure-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "manufactured-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preprocessing Strategy\n",
    "# text_cols= [\"text\",\"title\"]\n",
    "\n",
    "# for t_c in text_cols:\n",
    "#     t_df = Tokenizer(inputCol = t_c, outputCol = \"token_\"+t_c).transform(t_df)\n",
    "#     t_df = StopWordsRemover(inputCol = \"token_\"+t_c, outputCol = t_c+\"_f_token\").transform(t_df)\n",
    "    \n",
    "#     for i in range(1,4):\n",
    "#         t_df= NGram(n=i, inputCol=t_c+\"_f_token\", outputCol=t_c+\"_ngram_\"+str(i)).transform(t_df)\n",
    "        \n",
    "#         #CountVectorizer for each ngram col\n",
    "#         c_v = CountVectorizer(inputCol= t_c+\"_ngram_\"+str(i), outputCol=t_c+\"_ngram_\"+str(i)+\"cvfeat\",\n",
    "#                               minDF=2.0, vocabSize=2**14)\n",
    "        \n",
    "#         #indexer = StringIndexer(inputCol=t_c+\"_ngram_\"+str(i), outputCol=t_c+\"_ngram_\"+str(i)+\"ix_feat\")\n",
    "#         #display(t_c+\"_ngram_\"+str(i), t_c+\"_ngram_\"+str(i)+\"cvfeat\")\n",
    "        \n",
    "#         #m_ = c_v.fit(t_df) # c_v.fit is failing\n",
    "#         #t_df = m_.tranform(t_df)\n",
    "        \n",
    "\n",
    "#     t_df = t_df.drop(\"token_\"+t_c)\n",
    "\n",
    "\n",
    "# #t_df.show()\n",
    "\n",
    "\n",
    "# # def build_ngrams_wocs(inputCol=[\"text\",\"target\"], n=3):\n",
    "# #     tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "# #     ngrams = [\n",
    "# #         NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "# #         for i in range(1, n + 1)\n",
    "# #     ]\n",
    "\n",
    "# #     cv = [\n",
    "# #         CountVectorizer(vocabSize=5460,inputCol=\"{0}_grams\".format(i),\n",
    "# #             outputCol=\"{0}_tf\".format(i))\n",
    "# #         for i in range(1, n + 1)\n",
    "# #     ]\n",
    "# #     idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "# #     assembler = [VectorAssembler(\n",
    "# #         inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "# #         outputCol=\"features\"\n",
    "# #     )]\n",
    "# #     label_stringIdx = [StringIndexer(inputCol = \"target\", outputCol = \"label\")]\n",
    "# #     lr = [LogisticRegression(maxIter=100)]\n",
    "# #     return Pipeline(stages=tokenizer + ngrams + cv + idf+ assembler + label_stringIdx+lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-visitor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "arbitrary-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-optimization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "southern-chain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string, s: string, ss: array<string>, sss: array<string>, ssss: array<string>, ssss_1gram: array<string>, ssss_2gram: array<string>, ssss_3gram: array<string>, 3gram_feats: vector]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_v = CountVectorizer(inputCol=\"ssss_3gram\", outputCol=\"3gram_feats\",\n",
    "                     minDF=2.0, vocabSize=2**14)\n",
    "\n",
    "# _p = {'inputCol':\"title_ngram_1\", 'outputCol':\"title_ngram_1_cv\",\n",
    "#       'minDF':2.0, 'vocabSize':2**14}\n",
    "\n",
    "#m_ = c_v.fit(t_df, params={'inputCol': 'text_ngram_1'})\n",
    "mdl = c_v.fit(t_df)\n",
    "\n",
    "t_df=mdl.transform(t_df)\n",
    "t_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "coral-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame(\n",
    "#     [(0, [\"a\", \"b a\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "#     [\"label\", \"raw\"])\n",
    "# cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
    "# model = cv.fit(df)\n",
    "# model.transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "superb-framing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- ss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- sss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ssss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ssss_1gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- ssss_2gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- ssss_3gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3gram_feats: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " ['id',\n",
       "  'title',\n",
       "  'text',\n",
       "  'label',\n",
       "  's',\n",
       "  'ss',\n",
       "  'sss',\n",
       "  'ssss',\n",
       "  'ssss_1gram',\n",
       "  'ssss_2gram',\n",
       "  'ssss_3gram',\n",
       "  '3gram_feats'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_df.printSchema(), t_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sufficient-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module pyspark.ml.feature:\n",
      "\n",
      "class CountVectorizer(pyspark.ml.wrapper.JavaEstimator, _CountVectorizerParams, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  Extracts a vocabulary from document collections and generates a :py:attr:`CountVectorizerModel`.\n",
      " |  \n",
      " |  >>> df = spark.createDataFrame(\n",
      " |  ...    [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
      " |  ...    [\"label\", \"raw\"])\n",
      " |  >>> cv = CountVectorizer(inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> model = cv.fit(df)\n",
      " |  >>> model.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  >>> sorted(model.vocabulary) == ['a', 'b', 'c']\n",
      " |  True\n",
      " |  >>> countVectorizerPath = temp_path + \"/count-vectorizer\"\n",
      " |  >>> cv.save(countVectorizerPath)\n",
      " |  >>> loadedCv = CountVectorizer.load(countVectorizerPath)\n",
      " |  >>> loadedCv.getMinDF() == cv.getMinDF()\n",
      " |  True\n",
      " |  >>> loadedCv.getMinTF() == cv.getMinTF()\n",
      " |  True\n",
      " |  >>> loadedCv.getVocabSize() == cv.getVocabSize()\n",
      " |  True\n",
      " |  >>> modelPath = temp_path + \"/count-vectorizer-model\"\n",
      " |  >>> model.save(modelPath)\n",
      " |  >>> loadedModel = CountVectorizerModel.load(modelPath)\n",
      " |  >>> loadedModel.vocabulary == model.vocabulary\n",
      " |  True\n",
      " |  >>> fromVocabModel = CountVectorizerModel.from_vocabulary([\"a\", \"b\", \"c\"],\n",
      " |  ...     inputCol=\"raw\", outputCol=\"vectors\")\n",
      " |  >>> fromVocabModel.transform(df).show(truncate=False)\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |label|raw            |vectors                  |\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  |0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      " |  |1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      " |  +-----+---------------+-------------------------+\n",
      " |  ...\n",
      " |  \n",
      " |  .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      _CountVectorizerParams\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      __init__(self, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18, binary=False,                 inputCol=None,outputCol=None)\n",
      " |  \n",
      " |  setBinary(self, value)\n",
      " |      Sets the value of :py:attr:`binary`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  setMaxDF(self, value)\n",
      " |      Sets the value of :py:attr:`maxDF`.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  setMinDF(self, value)\n",
      " |      Sets the value of :py:attr:`minDF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setMinTF(self, value)\n",
      " |      Sets the value of :py:attr:`minTF`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setParams(self, minTF=1.0, minDF=1.0, maxDF=9223372036854775807, vocabSize=262144, binary=False, inputCol=None, outputCol=None)\n",
      " |      setParams(self, minTF=1.0, minDF=1.0, maxDF=2 ** 63 - 1, vocabSize=1 << 18, binary=False,                  inputCol=None, outputCol=None)\n",
      " |      Set the params for the CountVectorizer\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setVocabSize(self, value)\n",
      " |      Sets the value of :py:attr:`vocabSize`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  getBinary(self)\n",
      " |      Gets the value of binary or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  getMaxDF(self)\n",
      " |      Gets the value of maxDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  getMinDF(self)\n",
      " |      Gets the value of minDF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getMinTF(self)\n",
      " |      Gets the value of minTF or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getVocabSize(self)\n",
      " |      Gets the value of vocabSize or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CountVectorizerParams:\n",
      " |  \n",
      " |  binary = Param(parent='undefined', name='binary', doc='Bi...vents rath...\n",
      " |  \n",
      " |  maxDF = Param(parent='undefined', name='maxDF', doc='Spe...ts the term...\n",
      " |  \n",
      " |  minDF = Param(parent='undefined', name='minDF', doc='Spe...pecifies th...\n",
      " |  \n",
      " |  minTF = Param(parent='undefined', name='minTF', doc=\"Fil...rModel and ...\n",
      " |  \n",
      " |  vocabSize = Param(parent='undefined', name='vocabSize', doc='max size ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n",
      " |                     param maps is given, this calls fit on each param map and returns a list of\n",
      " |                     models.\n",
      " |      :returns: fitted model(s)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n",
      " |      :param paramMaps: A Sequence of param maps.\n",
      " |      :return: A thread safe iterable which contains one model for each param map. Each\n",
      " |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |               using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |      \n",
      " |      .. note:: DeveloperApi\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#text_tokenizer = Tokenizer(inputCol = 'text', outputCol = 'text_words')\n",
    "#ttl_tokenizer = Tokenizer(inputCol = 'title', outputCol = 'ttl_words')\n",
    "#regex_tokenizer = RegexTokenizer(inputCol= 'sentence', outputCol= 'words', pattern='\\\\W')\n",
    "#t_df = tokenizer.transform(t_df)\n",
    "# t_df = text_tokenizer.transform(ttl_tokenizer.transform(t_df))\n",
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "alleged-promise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Feature building\n",
    "\n",
    "# fcols = [\n",
    "# #  'text_f_token',\n",
    "#   'text_ngram_1',\n",
    "#   'text_ngram_2',\n",
    "#   'text_ngram_3',\n",
    "# #  'title_f_token',\n",
    "#   'title_ngram_1',\n",
    "#   'title_ngram_2',\n",
    "#   'title_ngram_3',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# cv = CountVectorizer(inputCol=\"text_ngram_2\", outputCol=\"features\")\n",
    "# t_df=cv.fit(t_df).transform(t_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ohe = OneHotEncoderEstimator(\n",
    "#     inputCols= fcols,\n",
    "#     outputCols=[\"feat_\"+s for s in fcols])\n",
    "\n",
    "#f_df=ohe.fit(t_df).transform(t_df)\n",
    "    \n",
    "#f_df=cv.fit(t_df).transform(t_df)\n",
    "#OneHotEncoderEstimator\n",
    "#hashing_tf = HashingTF(inputCol='words', outputCol='rawFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "wound-study",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, title: string, text: string, label: string, s: string, ss: array<string>, sss: array<string>, ssss: array<string>, ssss_1gram: array<string>, ssss_2gram: array<string>, ssss_3gram: array<string>, 3gram_feats: vector, label_index: double]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "t_df = indexer.fit(t_df).transform(t_df)\n",
    "t_df.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aware-furniture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- s: string (nullable = true)\n",
      " |-- ss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- sss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ssss: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ssss_1gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- ssss_2gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- ssss_3gram: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- 3gram_feats: vector (nullable = true)\n",
      " |-- label_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# idf= IDF(inputCol='rawFeatures', outputCol='features')\n",
    "# idf_model = idf.fit(featurized_df)\n",
    "# rescaled_df = idf_model.transform(featurized_df)\n",
    "\n",
    "# rescaled_df.show(truncate=False)\n",
    "\n",
    "t_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "rough-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class NaiveBayes in module pyspark.ml.classification:\n",
      "\n",
      "class NaiveBayes(pyspark.ml.wrapper.JavaEstimator, pyspark.ml.param.shared.HasFeaturesCol, pyspark.ml.param.shared.HasLabelCol, pyspark.ml.param.shared.HasPredictionCol, pyspark.ml.param.shared.HasProbabilityCol, pyspark.ml.param.shared.HasRawPredictionCol, pyspark.ml.param.shared.HasThresholds, pyspark.ml.param.shared.HasWeightCol, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  Naive Bayes Classifiers.\n",
      " |  It supports both Multinomial and Bernoulli NB. `Multinomial NB\n",
      " |  <http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html>`_\n",
      " |  can handle finitely supported discrete data. For example, by converting documents into\n",
      " |  TF-IDF vectors, it can be used for document classification. By making every vector a\n",
      " |  binary (0/1) data, it can also be used as `Bernoulli NB\n",
      " |  <http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html>`_.\n",
      " |  The input feature values must be nonnegative.\n",
      " |  \n",
      " |  >>> from pyspark.sql import Row\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     Row(label=0.0, weight=0.1, features=Vectors.dense([0.0, 0.0])),\n",
      " |  ...     Row(label=0.0, weight=0.5, features=Vectors.dense([0.0, 1.0])),\n",
      " |  ...     Row(label=1.0, weight=1.0, features=Vectors.dense([1.0, 0.0]))])\n",
      " |  >>> nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\", weightCol=\"weight\")\n",
      " |  >>> model = nb.fit(df)\n",
      " |  >>> model.pi\n",
      " |  DenseVector([-0.81..., -0.58...])\n",
      " |  >>> model.theta\n",
      " |  DenseMatrix(2, 2, [-0.91..., -0.51..., -0.40..., -1.09...], 1)\n",
      " |  >>> test0 = sc.parallelize([Row(features=Vectors.dense([1.0, 0.0]))]).toDF()\n",
      " |  >>> result = model.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  1.0\n",
      " |  >>> result.probability\n",
      " |  DenseVector([0.32..., 0.67...])\n",
      " |  >>> result.rawPrediction\n",
      " |  DenseVector([-1.72..., -0.99...])\n",
      " |  >>> test1 = sc.parallelize([Row(features=Vectors.sparse(2, [0], [1.0]))]).toDF()\n",
      " |  >>> model.transform(test1).head().prediction\n",
      " |  1.0\n",
      " |  >>> nb_path = temp_path + \"/nb\"\n",
      " |  >>> nb.save(nb_path)\n",
      " |  >>> nb2 = NaiveBayes.load(nb_path)\n",
      " |  >>> nb2.getSmoothing()\n",
      " |  1.0\n",
      " |  >>> model_path = temp_path + \"/nb_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = NaiveBayesModel.load(model_path)\n",
      " |  >>> model.pi == model2.pi\n",
      " |  True\n",
      " |  >>> model.theta == model2.theta\n",
      " |  True\n",
      " |  >>> nb = nb.setThresholds([0.01, 10.00])\n",
      " |  >>> model3 = nb.fit(df)\n",
      " |  >>> result = model3.transform(test0).head()\n",
      " |  >>> result.prediction\n",
      " |  0.0\n",
      " |  \n",
      " |  .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      NaiveBayes\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasProbabilityCol\n",
      " |      pyspark.ml.param.shared.HasRawPredictionCol\n",
      " |      pyspark.ml.param.shared.HasThresholds\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)\n",
      " |      __init__(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0,                  modelType=\"multinomial\", thresholds=None, weightCol=None)\n",
      " |  \n",
      " |  getModelType(self)\n",
      " |      Gets the value of modelType or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  getSmoothing(self)\n",
      " |      Gets the value of smoothing or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setModelType(self, value)\n",
      " |      Sets the value of :py:attr:`modelType`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setParams(self, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability', rawPredictionCol='rawPrediction', smoothing=1.0, modelType='multinomial', thresholds=None, weightCol=None)\n",
      " |      setParams(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", smoothing=1.0,                   modelType=\"multinomial\", thresholds=None, weightCol=None)\n",
      " |      Sets params for Naive Bayes.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  setSmoothing(self, value)\n",
      " |      Sets the value of :py:attr:`smoothing`.\n",
      " |      \n",
      " |      .. versionadded:: 1.5.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  modelType = Param(parent='undefined', name='modelType', doc=...d optio...\n",
      " |  \n",
      " |  smoothing = Param(parent='undefined', name='smoothing', doc=...thing p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaEstimator:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset, params=None)\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params. If a list/tuple of\n",
      " |                     param maps is given, this calls fit on each param map and returns a list of\n",
      " |                     models.\n",
      " |      :returns: fitted model(s)\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  fitMultiple(self, dataset, paramMaps)\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`.\n",
      " |      :param paramMaps: A Sequence of param maps.\n",
      " |      :return: A thread safe iterable which contains one model for each param map. Each\n",
      " |               call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |               using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |      \n",
      " |      .. note:: DeveloperApi\n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self)\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  setFeaturesCol(self, value)\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self)\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  setLabelCol(self, value)\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self)\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  setPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  getProbabilityCol(self)\n",
      " |      Gets the value of probabilityCol or its default value.\n",
      " |  \n",
      " |  setProbabilityCol(self, value)\n",
      " |      Sets the value of :py:attr:`probabilityCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasProbabilityCol:\n",
      " |  \n",
      " |  probabilityCol = Param(parent='undefined', name='probabilityCol',...at...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  getRawPredictionCol(self)\n",
      " |      Gets the value of rawPredictionCol or its default value.\n",
      " |  \n",
      " |  setRawPredictionCol(self, value)\n",
      " |      Sets the value of :py:attr:`rawPredictionCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRawPredictionCol:\n",
      " |  \n",
      " |  rawPredictionCol = Param(parent='undefined', name='rawPredictionCol......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasThresholds:\n",
      " |  \n",
      " |  getThresholds(self)\n",
      " |      Gets the value of thresholds or its default value.\n",
      " |  \n",
      " |  setThresholds(self, value)\n",
      " |      Sets the value of :py:attr:`thresholds`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasThresholds:\n",
      " |  \n",
      " |  thresholds = Param(parent='undefined', name='thresholds', doc...y of t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self)\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  setWeightCol(self, value)\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(NaiveBayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "geological-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,StringType,true),StructField(title,StringType,true),StructField(text,StringType,true),StructField(label,StringType,true),StructField(s,StringType,true),StructField(ss,ArrayType(StringType,true),true),StructField(sss,ArrayType(StringType,true),true),StructField(ssss,ArrayType(StringType,true),true),StructField(ssss_1gram,ArrayType(StringType,false),true),StructField(ssss_2gram,ArrayType(StringType,false),true),StructField(ssss_3gram,ArrayType(StringType,false),true),StructField(3gram_feats,VectorUDT,true),StructField(label_index,DoubleType,false)))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESSED_DF_CSV_PATH=\"file:///home/hadoop/cb/pre_train.csv\"\n",
    "#t_df.repartition(1).write.format('com.databricks.spark.csv').save(PREPROCESSED_DF_CSV_PATH,header = 'true')\n",
    "\n",
    "t_df.repartition(1).write.json(PREPROCESSED_DF_CSV_PATH,mode=\"overwrite\")\n",
    "\n",
    "t_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "parallel-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method json in module pyspark.sql.readwriter:\n",
      "\n",
      "json(path, mode=None, compression=None, dateFormat=None, timestampFormat=None, lineSep=None, encoding=None) method of pyspark.sql.readwriter.DataFrameWriter instance\n",
      "    Saves the content of the :class:`DataFrame` in JSON format\n",
      "    (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\n",
      "    specified path.\n",
      "    \n",
      "    :param path: the path in any Hadoop supported file system\n",
      "    :param mode: specifies the behavior of the save operation when data already exists.\n",
      "    \n",
      "        * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
      "        * ``overwrite``: Overwrite existing data.\n",
      "        * ``ignore``: Silently ignore this operation if data already exists.\n",
      "        * ``error`` or ``errorifexists`` (default case): Throw an exception if data already                 exists.\n",
      "    :param compression: compression codec to use when saving to file. This can be one of the\n",
      "                        known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
      "                        snappy and deflate).\n",
      "    :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      "                       follow the formats at ``java.text.SimpleDateFormat``. This\n",
      "                       applies to date type. If None is set, it uses the\n",
      "                       default value, ``yyyy-MM-dd``.\n",
      "    :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      "                            formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      "                            This applies to timestamp type. If None is set, it uses the\n",
      "                            default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      "    :param encoding: specifies encoding (charset) of saved json files. If None is set,\n",
      "                    the default UTF-8 charset will be used.\n",
      "    :param lineSep: defines the line separator that should be used for writing. If None is\n",
      "                    set, it uses the default value, ``\\n``.\n",
      "    \n",
      "    >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(t_df.write.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"fail\")\n",
    "\n",
    "train, test = t_df.randomSplit([.8,.2], seed = 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = NaiveBayes(modelType= \"multinomial\", labelCol=\"label_index\", featuresCol =\"3gram_feats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_model= NB.fit(train)\n",
    "nb_predictions= nb_model.transform(test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Py4JJavaError: An error occurred while calling o459.fit.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 23, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol= \"label_index\", predictionCol = \"prediction\", \n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
    "\n",
    "print(\"Accuracy : \", nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://spark.apache.org/docs/2.2.0/ml-pipeline.html\n",
    "\n",
    "Plan:\n",
    "    - use Pipeline([transformer_n, estimator_n...])\n",
    "    - Improve preprocessing (punctuation, special characters, stemming)\n",
    "    - Include transformers and estimators for each n_gram length\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-terrorist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-tobago",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
